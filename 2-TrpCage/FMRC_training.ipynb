{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plumed\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import MDAnalysis as md\n",
    "from MDAnalysis.analysis import distances\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "import deeptime\n",
    "from deeptime.decomposition import TICA\n",
    "from deeptime.covariance import KoopmanWeightingEstimator\n",
    "from deeptime.clustering import MiniBatchKMeans\n",
    "from deeptime.markov import TransitionCountEstimator\n",
    "from deeptime.markov.msm import MaximumLikelihoodMSM\n",
    "from deeptime.markov.msm import BayesianMSM\n",
    "from deeptime.plots import plot_implied_timescales\n",
    "from deeptime.util.validation import implied_timescales\n",
    "from deeptime.plots.chapman_kolmogorov import plot_ck_test\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataset import random_split\n",
    "from snrv import Snrv, load_snrv\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5987b8f-0481-4428-a571-8cb8a01570d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS TO CLEAR CUDA MEMORY\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffe9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f40a50-94e9-46a9-b02e-8d5409607ed0",
   "metadata": {},
   "source": [
    "#### 1. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f0450-dd33-4eec-b1ef-62f3689f0ac8",
   "metadata": {},
   "source": [
    "##### 1.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe99e22-e339-409b-a3c0-a1d4d10fed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection functions\n",
    "\n",
    "def select_dihedrals(universe,dihedral_type,start_res,end_res):\n",
    "    dihedrals={}\n",
    "    if 'phi' in dihedral_type:\n",
    "        dihedrals['phi'] = []\n",
    "        for i in range(start_res,end_res+1):\n",
    "            if i != universe.residues.resids[0]:\n",
    "                dihedrals['phi'].append(i)\n",
    "\n",
    "    if 'psi' in dihedral_type:\n",
    "        dihedrals['psi'] = []\n",
    "        for i in range(start_res,end_res+1):\n",
    "            if i != universe.residues.resids[-1]:\n",
    "                dihedrals['psi'].append(i)\n",
    "\n",
    "    if 'omega' in dihedral_type:\n",
    "        dihedrals['omega'] = []\n",
    "        for i in range(start_res,end_res+1):\n",
    "            if i != universe.residues.resids[0]:\n",
    "                dihedrals['omega'].append(i)\n",
    "\n",
    "    if 'chi1' in dihedral_type:\n",
    "        dihedrals['chi1']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] not in ['GLY','ALA']:\n",
    "                dihedrals['chi1'].append(i)\n",
    "\n",
    "    if 'chi2' in dihedral_type:\n",
    "        dihedrals['chi2']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] not in ['GLY','ALA','CYS','SER','THR','VAL']:\n",
    "                dihedrals['chi2'].append(i)\n",
    "\n",
    "    if 'chi3' in dihedral_type:\n",
    "        dihedrals['chi3']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] in ['ARG','GLN','GLU','LYS','MET']:\n",
    "                dihedrals['chi3'].append(i)\n",
    "\n",
    "    if 'chi4' in dihedral_type:\n",
    "        dihedrals['chi4']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] in ['ARG','LYS']:\n",
    "                dihedrals['chi4'].append(i)\n",
    "    return dihedrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b2011-c82d-4500-9eb8-51d16c1c6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_stride = 1                               # stride for trajectories output\n",
    "\n",
    "u = md.Universe('traj_and_dat/input.pdb')\n",
    "nitrogen = u.select_atoms('name N')\n",
    "oxygen = u.select_atoms('name O')\n",
    "calpha = u.select_atoms('name CA')\n",
    "cbeta = u.select_atoms('name CB')\n",
    "cgamma = u.select_atoms('name CG or name CG1 or name CG2')\n",
    "pairs = {'hbond':[],'CA':[],'CB':[],'CG':[]}\n",
    "nitrogen_id = nitrogen.ids\n",
    "oxygen_id = oxygen.ids\n",
    "calpha_id = calpha.ids\n",
    "cbeta_id = cbeta.ids\n",
    "cgamma_id = cgamma.ids\n",
    "pairs['hbond'] = list(itertools.product(nitrogen_id,oxygen_id))\n",
    "pairs['CA'] = list(itertools.combinations(calpha_id,2))\n",
    "pairs['CB'] = list(itertools.combinations(cbeta_id,2))\n",
    "pairs['CG'] = list(itertools.combinations(cgamma_id,2))\n",
    "dihedrals = select_dihedrals(u,['phi','psi','chi1','chi2','chi3','chi4'],1,20)\n",
    "    \n",
    "# Write features into plumed file for TICA. e^-d are introduced as compensating extra features input due to its linearity.\n",
    "with open('traj_and_dat/features_tica.dat','w+') as f:\n",
    "    f.writelines('MOLINFO STRUCTURE=input.pdb\\n')\n",
    "    count = 0\n",
    "    for pair in pairs['hbond']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for pair in pairs['CA']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for pair in pairs['CB']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for pair in pairs['CG']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for dihedral in dihedrals:\n",
    "        for resid in dihedrals[dihedral]:\n",
    "            f.writelines('{dihedral}-{resid}: TORSION ATOMS=@{dihedral}-{resid}\\n'.format(dihedral=dihedral,resid=resid))\n",
    "            f.writelines('sin{dihedral}-{resid}: CUSTOM ARG={dihedral}-{resid} FUNC=sin(x) PERIODIC=NO\\n'.format(dihedral=dihedral,resid=resid))\n",
    "            f.writelines('cos{dihedral}-{resid}: CUSTOM ARG={dihedral}-{resid} FUNC=cos(x) PERIODIC=NO\\n'.format(dihedral=dihedral,resid=resid))\n",
    "        f.writelines('\\n')\n",
    "    f.writelines('PRINT ARG=* STRIDE={traj_stride} FILE=COLVAR'.format(traj_stride=traj_stride))\n",
    "\n",
    "# Write features into plumed file for SRV and RCFlow\n",
    "with open('traj_and_dat/features.dat','w+') as f:\n",
    "    f.writelines('MOLINFO STRUCTURE=input.pdb\\n')\n",
    "    count = 0\n",
    "    for pair in pairs['hbond']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        #f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for pair in pairs['CA']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        #f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for pair in pairs['CB']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        #f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for pair in pairs['CG']:\n",
    "        atom1 = pair[0]\n",
    "        atom2 = pair[1]\n",
    "        count = count + 1\n",
    "        f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        #f.writelines('epair{count}: CUSTOM ARG=pair{count} FUNC=e^-x PERIODIC=NO\\n'.format(count=count))\n",
    "    for dihedral in dihedrals:\n",
    "        for resid in dihedrals[dihedral]:\n",
    "            f.writelines('{dihedral}-{resid}: TORSION ATOMS=@{dihedral}-{resid}\\n'.format(dihedral=dihedral,resid=resid))\n",
    "            f.writelines('sin{dihedral}-{resid}: CUSTOM ARG={dihedral}-{resid} FUNC=sin(x) PERIODIC=NO\\n'.format(dihedral=dihedral,resid=resid))\n",
    "            f.writelines('cos{dihedral}-{resid}: CUSTOM ARG={dihedral}-{resid} FUNC=cos(x) PERIODIC=NO\\n'.format(dihedral=dihedral,resid=resid))\n",
    "        f.writelines('\\n')\n",
    "    f.writelines('PRINT ARG=* STRIDE={traj_stride} FILE=COLVAR'.format(traj_stride=traj_stride))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bd127-49ec-45b2-b9d8-a84bdc7e45dc",
   "metadata": {},
   "source": [
    "#### 2. Load featurized trajectories (without $e^{-d}$ functions and sin/cos functions) and Train FMRC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88869796-3669-4afb-8acc-90291c583bc4",
   "metadata": {},
   "source": [
    "##### 2.1 FMRC code and read features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5b714-0138-482a-94d9-6c157c5296f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCflow\n",
    "class GaussianPrior(nn.Module):\n",
    "    def __init__(self, mean_value, std_value):\n",
    "        super().__init__()\n",
    "        self.mean_ = nn.Parameter(torch.tensor(mean_value, dtype=torch.float32), requires_grad=False)\n",
    "        self.std_ = nn.Parameter(torch.tensor(std_value, dtype=torch.float32), requires_grad=False)\n",
    "\n",
    "    def forward(self, size):\n",
    "        samples = self.mean_ + self.std_ * torch.randn(size).to(self.mean_.device)\n",
    "        return samples\n",
    "\n",
    "    def sample_like(self, x):\n",
    "        size = x.size()\n",
    "        samples = self.forward(size).to(x.device)\n",
    "        return samples\n",
    "\n",
    "\n",
    "class RCFlow(nn.Module):\n",
    "    def __init__(self,input_size,latent_size,encoder_hidden_size,encode_state_label,hidden_size,hidden_depth,activation,sigma,\n",
    "                 learning_rate,lr_decay,lr_decay_stepsize,val_frac,batch_size,batchnorm,n_epochs,device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Neural network related\n",
    "        self.input_size = input_size               # No. of features\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_depth = hidden_depth\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "        self.encode_state_label = encode_state_label\n",
    "        self.sigma = sigma                         # the gaussian width of flow matching vector field sample, \n",
    "                                                   # serves as a regularization factor\n",
    "\n",
    "        # Training related\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_decay_stepsize = lr_decay_stepsize\n",
    "        self.val_frac = val_frac\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = device\n",
    "\n",
    "        # Cached attributes\n",
    "        self.encoder = None\n",
    "        self.L_vector_field = None\n",
    "        self.D_vector_field = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.train_loss = None\n",
    "        self.validation_loss = None\n",
    "        \n",
    "        #####\n",
    "        \n",
    "        ### Encoder x --> r(x) as a feed forward nn\n",
    "        self.encoder = []\n",
    "        # first layer\n",
    "        self.encoder.append(nn.Linear(self.input_size,self.encoder_hidden_size))\n",
    "        # Insert dropout/batchnorm here\n",
    "        # Activation\n",
    "        self.encoder.append(self.activation)\n",
    "        if self.batchnorm == True:\n",
    "            self.encoder.append(nn.BatchNorm1d(self.encoder_hidden_size))\n",
    "        # middle layers\n",
    "        for i in range(self.hidden_depth-1):\n",
    "            self.encoder.append(nn.Linear(self.encoder_hidden_size,self.encoder_hidden_size))\n",
    "            self.encoder.append(self.activation)\n",
    "            if self.batchnorm == True:\n",
    "                self.encoder.append(nn.BatchNorm1d(self.encoder_hidden_size))\n",
    "        \n",
    "        # final output layer\n",
    "        self.encoder.append(nn.Linear(self.encoder_hidden_size,self.latent_size))\n",
    "        \n",
    "        if self.encode_state_label == True:\n",
    "            self.encoder.append(nn.Softmax())\n",
    "            \n",
    "        self.encoder = nn.Sequential(*self.encoder).to(self.device)\n",
    "\n",
    "        #####\n",
    "\n",
    "        ### nn that represents the lumpability vector field u(r(x),y,t)\n",
    "        self.L_vector_field = []\n",
    "        L_input_size = self.latent_size + self.input_size + 1\n",
    "        \n",
    "        # first layer\n",
    "        self.L_vector_field.append(nn.Linear(L_input_size,self.hidden_size))\n",
    "        self.L_vector_field.append(self.activation)\n",
    "        if self.batchnorm == True:\n",
    "            self.L_vector_field.append(nn.BatchNorm1d(self.hidden_size))\n",
    "        # middle layers\n",
    "        for i in range(self.hidden_depth-1):\n",
    "            self.L_vector_field.append(nn.Linear(self.hidden_size,self.hidden_size))\n",
    "            self.L_vector_field.append(self.activation)\n",
    "            if self.batchnorm == True:\n",
    "                self.L_vector_field.append(nn.BatchNorm1d(self.hidden_size))\n",
    "        # final output layer\n",
    "        self.L_vector_field.append(nn.Linear(self.hidden_size,self.input_size))\n",
    "        self.L_vector_field = nn.Sequential(*self.L_vector_field).to(self.device)\n",
    "\n",
    "        #####\n",
    "        \n",
    "        ### nn that represents the decomposibility vector field u(r(y),x,t)\n",
    "        self.D_vector_field = []\n",
    "        \n",
    "        D_input_size = self.latent_size + self.input_size + 1\n",
    "        \n",
    "        # first layer\n",
    "        self.D_vector_field.append(nn.Linear(D_input_size,self.hidden_size))\n",
    "        self.D_vector_field.append(self.activation)\n",
    "        if self.batchnorm == True:\n",
    "            self.D_vector_field.append(nn.BatchNorm1d(self.hidden_size))\n",
    "        # middle layers\n",
    "        for i in range(self.hidden_depth-1):\n",
    "            self.D_vector_field.append(nn.Linear(self.hidden_size,self.hidden_size))\n",
    "            self.D_vector_field.append(self.activation)\n",
    "            if self.batchnorm == True:\n",
    "                self.D_vector_field.append(nn.BatchNorm1d(self.hidden_size))\n",
    "        # final output layer\n",
    "        self.D_vector_field.append(nn.Linear(self.hidden_size,self.input_size))\n",
    "        self.D_vector_field = nn.Sequential(*self.D_vector_field).to(self.device)\n",
    "\n",
    "        #####\n",
    "        \n",
    "    def encode(self,x):\n",
    "        # Here, x is the variable to be encoded i.e. for lumpability x:= x for decomposibility x:= y\n",
    "        # encode x --> r:=r(x)\n",
    "        r = self.encoder(x)\n",
    "        return r\n",
    "\n",
    "    def sample_from_prior(self,x):\n",
    "        # Sample x/y (D_loss/L_loss) from prior, prior_sample should have shape (batch_size,input_size)\n",
    "        # Since this is just a gaussian with mean & s.t.d from all data, x/y can share the same prior\n",
    "        prior_sample = self.prior.sample_like(x)\n",
    "        return prior_sample\n",
    "\n",
    "    def sample_t(self,x):\n",
    "        # Sample t = [t_1,...t_B], t should have shape (batch_size,)\n",
    "        t = torch.rand_like(x[:,:1])\n",
    "        return t\n",
    "\n",
    "    def sample_x_t(self,x,t,prior_sample): \n",
    "        # Sample x_t/y_t, the 'location' of x/y after 'time' t in the vector field\n",
    "        x_t = x * t + (1-t) * prior_sample + torch.randn_like(x) * self.sigma\n",
    "        return x_t\n",
    "\n",
    "    def data_vector_field(self,x,prior_sample):\n",
    "        # Calculate v_t, the 'data vector field' that we want to match our 'neural network vector field' with\n",
    "        v_t = x - prior_sample\n",
    "        return v_t\n",
    "\n",
    "    def L_loss(self,x,y,t,rx=None):\n",
    "        # Step 1: encode x --> r(x)\n",
    "        if rx == None:\n",
    "            rx = self.encode(x)\n",
    "        \n",
    "        # Step 2: sample y from prior\n",
    "        prior_y = self.sample_from_prior(y)\n",
    "        \n",
    "        # Step 3: sample y_t, y at time t in the vector field\n",
    "        y_t = self.sample_x_t(y,t,prior_y)\n",
    "        \n",
    "        # Step 4: compute 'data vector field'\n",
    "        v_t = self.data_vector_field(y,prior_y)\n",
    "        \n",
    "        # Step 5: compute 'nn vector field'\n",
    "        u_input = torch.cat((rx,y_t,t),dim=-1)\n",
    "        # so that u_t is a function of rx,y_t,t only i.e. rx contains nearly same information as x\n",
    "        u_t = self.L_vector_field(u_input)\n",
    "        \n",
    "        # Step 6: compute flow matching loss\n",
    "        L_loss = torch.mean(torch.sum((u_t-v_t)**2,-1))\n",
    "        \n",
    "        return L_loss\n",
    "    \n",
    "    def D_loss(self,x,y,t,ry=None):\n",
    "        # Step 1: encode y --> r(y)\n",
    "        if ry == None:\n",
    "            ry = self.encode(y)\n",
    "        \n",
    "        # Step 2: sample x from prior\n",
    "        prior_x = self.sample_from_prior(x)\n",
    "        \n",
    "        # Step 3: sample x_t, x at time t in the vector field\n",
    "        x_t = self.sample_x_t(x,t,prior_x)\n",
    "        \n",
    "        # Step 4: compute 'data vector field'\n",
    "        v_t = self.data_vector_field(x,prior_x)\n",
    "        \n",
    "        # Step 5: compute 'nn vector field'\n",
    "        u_input = torch.cat((ry,x_t,t),dim=-1)\n",
    "        # so that u_t is a function of rx,y_t,t only i.e. rx contains nearly same information as x\n",
    "        u_t = self.L_vector_field(u_input)\n",
    "        \n",
    "        # Step 6: compute flow matching loss\n",
    "        D_loss = torch.mean(torch.sum((u_t-v_t)**2,-1))\n",
    "        \n",
    "        return D_loss\n",
    "\n",
    "    def fit(self,data,lagtime):\n",
    "        # NB: usually we use tica_output as data\n",
    "        # Initialize the gaussian prior\n",
    "        prior_mean = np.mean(np.concatenate(data),axis=0)\n",
    "        prior_sigma = np.std(np.concatenate(data),axis=0)\n",
    "        self.prior = GaussianPrior(prior_mean,prior_sigma).to(self.device)\n",
    "\n",
    "        # Create time-lagged dataset: this outputs a 3d numpy array with shape (no_frame,2,no_features)\n",
    "        # the second dimension represents the time-lagged pairs X_t,X_t+tau\n",
    "        dataset = create_timelagged_dataset(data,lagtime)\n",
    "        \n",
    "        # Create training set and validation set\n",
    "        n_pairs = len(dataset)\n",
    "        train_size = int((1-self.val_frac)*n_pairs)\n",
    "        val_size = n_pairs - train_size\n",
    "        train_data, val_data = random_split(dataset,[train_size,val_size])\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, batch_size = self.batch_size,shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_data, batch_size = self.batch_size)  \n",
    "        \n",
    "        # Training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.lr_decay_stepsize, gamma=self.lr_decay)\n",
    "        \n",
    "        train_loss = []\n",
    "        validation_loss = []\n",
    "        \n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            for epoch in range(1, n_epochs + 1):\n",
    "                train_loss_epoch = []\n",
    "                validation_loss_epoch = []\n",
    "                \n",
    "                # Training\n",
    "                for minibatch_data in train_loader:\n",
    "                    # Prepare x and y, should both in shape (batchsize,no_features)\n",
    "                    x = minibatch_data[:,0,:].to(self.device)\n",
    "                    y = minibatch_data[:,1,:].to(self.device)\n",
    "                    # Sample t\n",
    "                    t = self.sample_t(x)\n",
    "                    # Compute losses\n",
    "                    L_loss = self.L_loss(x,y,t)\n",
    "                    D_loss = self.D_loss(x,y,t)\n",
    "                    loss = L_loss + D_loss\n",
    "                    # back propagation\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    # Record current minibatch loss\n",
    "                    train_loss_epoch.append(loss.item())\n",
    "                train_loss_epoch = np.mean(train_loss_epoch)\n",
    "                train_loss.append(train_loss_epoch)\n",
    "                \n",
    "                # learning rate decay\n",
    "                self.scheduler.step()\n",
    "\n",
    "                # Validation\n",
    "                with torch.no_grad():\n",
    "                    for minibatch_data in val_loader:\n",
    "                        # Prepare x and y, should both in shape (batchsize,no_features)\n",
    "                        x = minibatch_data[:,0,:].to(self.device)\n",
    "                        y = minibatch_data[:,1,:].to(self.device)\n",
    "                        # Sample t\n",
    "                        t = self.sample_t(x)\n",
    "                        # Compute losses\n",
    "                        L_loss = self.L_loss(x,y,t)\n",
    "                        D_loss = self.D_loss(x,y,t)\n",
    "                        loss = L_loss + D_loss\n",
    "                        # Record current minibatch loss\n",
    "                        validation_loss_epoch.append(loss.item())\n",
    "                    validation_loss_epoch = np.mean(validation_loss_epoch)\n",
    "                    validation_loss.append(validation_loss_epoch)\n",
    "\n",
    "                print('Epoch {}: Train loss = {:.4f}, Validation loss = {:.4f}'.format(epoch,train_loss_epoch,validation_loss_epoch))\n",
    "\n",
    "        self.train_loss = train_loss\n",
    "        self.validation_loss = validation_loss\n",
    "                \n",
    "        return None\n",
    "        \n",
    "    def transform(self,data_concat):\n",
    "        r = self.encode(torch.tensor(data_concat,dtype=torch.float32).to(self.device)).cpu().detach().numpy()\n",
    "        return r\n",
    "\n",
    "    def save_model(self,filepath):\n",
    "        torch.save(self,filepath)\n",
    "        return None\n",
    "        \n",
    "    def plot_loss(self,sim_idx):\n",
    "        fig,ax = plt.subplots()\n",
    "        ax.plot(self.train_loss,label='train')\n",
    "        ax.plot(self.validation_loss,label='validation')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('figures/round{sim_idx}_rcflow_loss.png'.format(sim_idx=sim_idx-1),dpi=600)\n",
    "\n",
    "# We want to organize time-series trajectories of TICA eigenvectors into time-lagged pairs according to lagtime\n",
    "# i.e. tica_output + tica_output_supp --> data in RCflow, should be inshape (no_time_lagged_pairs, no_features * 2)\n",
    "def create_timelagged_dataset(tica_output,lagtime):\n",
    "    timelagged_dataset = []\n",
    "    tica_data = list(tica_output)\n",
    "    for tica_data_i in tica_data:\n",
    "        for i in range(tica_data_i.shape[0]-lagtime):\n",
    "            lagged_pair_i = np.vstack([tica_data_i[i],tica_data_i[i+lagtime]])\n",
    "            lagged_pair_i = np.array(lagged_pair_i)\n",
    "            timelagged_dataset.append(lagged_pair_i)\n",
    "    timelagged_dataset = torch.tensor(np.array(timelagged_dataset),dtype=torch.float32)\n",
    "    return timelagged_dataset\n",
    "\n",
    "def minmax_normalization(data,axis=0):\n",
    "    data_max = data.max(axis=0)\n",
    "    data_min = data.min(axis=0)\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    return normalized_data\n",
    "\n",
    "def calculate_nmicro(data_concat):\n",
    "    # Heuristic approach to determine cluster number from htmd \n",
    "    # https://github.com/Acellera/htmd/blob/master/htmd/adaptive/adaptivebandit.py\n",
    "    n_microstates = int(max(100, np.round(0.6 * np.log10(data_concat.shape[0] / 1000) * 1000 + 50)))\n",
    "    return n_microstates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd2957-6a36-4130-bd3f-a3c54b870dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to organize time-series trajectories of TICA eigenvectors into time-lagged pairs according to lagtime\n",
    "# i.e. tica_output + tica_output_supp --> data in RCflow, should be inshape (no_time_lagged_pairs, no_features * 2)\n",
    "def create_timelagged_dataset(tica_output,lagtime):\n",
    "    timelagged_dataset = []\n",
    "    tica_data = list(tica_output)\n",
    "    for tica_data_i in tica_data:\n",
    "        for i in range(tica_data_i.shape[0]-lagtime):\n",
    "            lagged_pair_i = np.vstack([tica_data_i[i],tica_data_i[i+lagtime]])\n",
    "            lagged_pair_i = np.array(lagged_pair_i)\n",
    "            timelagged_dataset.append(lagged_pair_i)\n",
    "    timelagged_dataset = torch.tensor(np.array(timelagged_dataset),dtype=torch.float32)\n",
    "    return timelagged_dataset\n",
    "\n",
    "def minmax_normalization(data,axis=0):\n",
    "    data_max = data.max(axis=0)\n",
    "    data_min = data.min(axis=0)\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    return normalized_data\n",
    "\n",
    "def calculate_nmicro(data_concat):\n",
    "    # Heuristic approach to determine cluster number from htmd \n",
    "    # https://github.com/Acellera/htmd/blob/master/htmd/adaptive/adaptivebandit.py\n",
    "    n_microstates = int(max(100, np.round(0.6 * np.log10(data_concat.shape[0] / 1000) * 1000 + 50)))\n",
    "    return n_microstates\n",
    "\n",
    "def rcflow_projection(tica_output_concat,normalized_r,stride,markersize,savefile=None,save=False):\n",
    "    \n",
    "    fig,ax = plt.subplots(ncols=2,nrows=2,figsize=(16,12))\n",
    "    \n",
    "    sc1 = ax[0,0].scatter(normalized_r[:,0][::stride],normalized_r[:,1][::stride],c=tica_output_concat[:,0][::stride],s=markersize)\n",
    "    ax[0,0].set_xlabel('RC 1')\n",
    "    ax[0,0].set_ylabel('RC 2')\n",
    "    ax[0,0].set_xticks(np.linspace(0,1,11))\n",
    "    ax[0,0].set_yticks(np.linspace(0,1,11))\n",
    "    fig.colorbar(sc1,ax=ax[0,0],label='TICA tIC1')\n",
    "    \n",
    "    sc2 = ax[0,1].scatter(normalized_r[:,0][::stride],normalized_r[:,1][::stride],c=tica_output_concat[:,1][::stride],s=markersize)\n",
    "    ax[0,1].set_xlabel('RC 1')\n",
    "    ax[0,1].set_ylabel('RC 2')\n",
    "    ax[0,1].set_xticks(np.linspace(0,1,11))\n",
    "    ax[0,1].set_yticks(np.linspace(0,1,11))\n",
    "    fig.colorbar(sc2,ax=ax[0,1],label='TICA tIC2')\n",
    "    \n",
    "    sc3 = ax[1,0].scatter(tica_output_concat[:,0][::stride],tica_output_concat[:,1][::stride],c=normalized_r[:,0][::stride],s=markersize)\n",
    "    ax[1,0].set_xlabel('TICA tIC 1')\n",
    "    ax[1,0].set_ylabel('TICA tIC 2')\n",
    "    fig.colorbar(sc3,ax=ax[1,0],label='RC 1',ticks=np.linspace(0,1,11))\n",
    "    \n",
    "    sc4 = ax[1,1].scatter(tica_output_concat[:,0][::stride],tica_output_concat[:,1][::stride],c=normalized_r[:,1][::stride],s=markersize)\n",
    "    ax[1,1].set_xlabel('TICA tIC 1')\n",
    "    ax[1,1].set_ylabel('TICA tIC 2')\n",
    "    fig.colorbar(sc4,ax=ax[1,1],label='RC 2',ticks=np.linspace(0,1,11))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save == True:\n",
    "        if savefile == None:\n",
    "            plt.savefig('figures/rcflow_projection/rcflow_projection.png',dpi=600)\n",
    "        else:\n",
    "            plt.savefig(savefile,dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "def run_TICA(data,lagtime,dim=None,var_cutoff=None,koopman=True):\n",
    "    tica = TICA(lagtime=lagtime,dim=dim,var_cutoff=var_cutoff)\n",
    "    if koopman == True:\n",
    "        koopman_estimator = KoopmanWeightingEstimator(lagtime=lagtime)\n",
    "        reweighting_model = koopman_estimator.fit(data).fetch_model()\n",
    "        tica = tica.fit(data, weights=reweighting_model).fetch_model()\n",
    "    else:\n",
    "        tica = tica.fit(data).fetch_model()\n",
    "    # tica is the data-fitted model, which contains eigenvalues and eigenvectors\n",
    "    # tica_output is the tranformed time-series data in TICA space in shape(traj_idx,no_frames,dim)\n",
    "    # tica_output_concat is tica_output in shape(traj_idx*no_frames,dim)\n",
    "    tica_output = tica.transform(data)\n",
    "        \n",
    "    return tica,tica_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d54141-8e38-45bd-a968-5eb92e321b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In principle, we do not need to include any e^-d functions, \n",
    "# since they are all functions of interatomic distances\n",
    "data = plumed.read_as_pandas('CV/COLVAR')\n",
    "data = data.drop(columns=['time'])\n",
    "columns = list(data.columns.values)\n",
    "for column in columns:\n",
    "    if column[:3] == 'phi' or column[:3] == 'psi' or column[:3] == 'chi' or column[:5] == 'omega':\n",
    "        data = data.drop(columns=[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312fb4a-0dcf-41ca-bf14-034dca106b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865a45a-c8da-43e4-a52e-cb6cef294d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf07fd3-ac83-41ca-ae9f-e417567f0b57",
   "metadata": {},
   "source": [
    "##### 2.2 Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa1d88-671a-426c-a30e-fcd6156fb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run control and data pre-processing\n",
    "dim = 20\n",
    "var_cutoff = None\n",
    "koopman = False\n",
    "\n",
    "# FMRC training hyperparamters\n",
    "rcflow_lagtime = 50                                \n",
    "latent_size = 2                                    # dimensions of r\n",
    "encoder_hidden_size = 256\n",
    "encode_state_label = False\n",
    "hidden_size = 256\n",
    "hidden_depth = 3\n",
    "activation = nn.ReLU()\n",
    "sigma = 0.001\n",
    "learning_rate = 0.001\n",
    "lr_decay = 0.1\n",
    "lr_decay_stepsize = 50\n",
    "val_frac = 0.1\n",
    "batch_size = 512\n",
    "batchnorm = False\n",
    "n_epochs = 100\n",
    "device = 'cuda'\n",
    "\n",
    "batchsize_transform = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa10e3-6a7a-4ec8-9b07-65a97c72999a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_supp = []\n",
    "train_loss_all = []\n",
    "validation_loss_all = []\n",
    "tica_output_supp = []\n",
    "# TICA pre-processing\n",
    "tica,tica_output = run_TICA(data,rcflow_lagtime,dim,var_cutoff,koopman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053a6bd-8824-4a45-8014-b983121fd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb4b0c-c0c9-4613-ab05-3d02036abb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = tica_output.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351b728-c75f-4efb-91c7-0456e18fd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_output_concat = np.concatenate(tica_output)\n",
    "tica_output_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44b492-5c4d-4ce7-ad27-a5bcaed545a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "#### Number of models to train\n",
    "no_models = 10\n",
    "\n",
    "for i in range(no_models):\n",
    "    rcflow = RCFlow(input_size,latent_size,encoder_hidden_size,encode_state_label,hidden_size,hidden_depth,activation,sigma,\n",
    "                    learning_rate,lr_decay,lr_decay_stepsize,val_frac,batch_size,batchnorm,n_epochs,device)\n",
    "    print(rcflow)\n",
    "    rcflow.fit(tica_output,rcflow_lagtime)\n",
    "    rcflow.save_model('models/rcflow-lag10-sincos-256hidden-{i}.pt'.format(i=i))\n",
    "\n",
    "    # Transform into RC space\n",
    "    \n",
    "    r = []\n",
    "    for j in range(no_iteration):\n",
    "        r_j = rcflow.transform(tica_output_concat[j*batchsize_transform:(j+1)*batchsize_transform])\n",
    "        r.append(r_j)\n",
    "    r = np.concatenate(r)\n",
    "    \n",
    "    # Normalize r for better clustering result\n",
    "    normalized_r = minmax_normalization(r,axis=0)\n",
    "    # Save figures for projection\n",
    "    rcflow_projection(tica_output_concat,normalized_r,stride,markersize,i=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182331f3-632d-4ca0-b65c-43bb4f37f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to load trained model\n",
    "#rcflow = torch.load('models/rcflow-lag10-sincos-2000hidden-1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421cb71-3f67-4ec8-bdd8-9c770bff9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into RC space\n",
    "no_iteration = tica_output_concat.shape[0]//batchsize_transform + 1\n",
    "\n",
    "r = []\n",
    "for j in range(no_iteration):\n",
    "    r_j = rcflow.transform(tica_output_concat[j*batchsize_transform:(j+1)*batchsize_transform])\n",
    "    r.append(r_j)\n",
    "r = np.concatenate(r)\n",
    "\n",
    "# Normalize r for better clustering result\n",
    "normalized_r = minmax_normalization(r,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71575a4c-857b-41c9-b3ce-2af1ed25ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 10\n",
    "markersize = 3\n",
    "rcflow_projection(tica_output_concat,normalized_r,stride,markersize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
